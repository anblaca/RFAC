{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xfmae-CojFDc"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RMSprop' from 'keras.optimizers' (C:\\Users\\Usuario\\anaconda3\\envs\\rasa\\lib\\site-packages\\keras\\optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningRateScheduler \u001b[38;5;28;01mas\u001b[39;00m LRS\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSprop\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceLROnPlateau\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, Adagrad\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RMSprop' from 'keras.optimizers' (C:\\Users\\Usuario\\anaconda3\\envs\\rasa\\lib\\site-packages\\keras\\optimizers.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization as BN\n",
    "from keras.layers import GaussianNoise as GN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler as LRS\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wp-RZqmkXWU"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC1dDGIrkYRU",
    "outputId": "6a820c96-8d3d-4c82-9dce-b773497c5819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 13s 0us/step\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#### LOAD AND TRANSFORM\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ry_stG2Lkhbq"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDdSMLVmtCSZ"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "  featurewise_center=True,\n",
    "  featurewise_std_normalization=True,\n",
    "  width_shift_range=0.2,\n",
    "  height_shift_range=0.2,\n",
    "  rotation_range=20,\n",
    "  zoom_range=[1.0,1.2],\n",
    "  horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gttaTutiSjdU"
   },
   "outputs": [],
   "source": [
    "datagen.fit(x_train)\n",
    "\n",
    "testdatagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,)\n",
    "\n",
    "testdatagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az1k8yaFkh1d"
   },
   "outputs": [],
   "source": [
    "## DEF A BLOCK CONV + BN + GN + MAXPOOL\n",
    "def CBGN(model,filters,ishape=0):\n",
    "  if (ishape!=0):\n",
    "    model.add(Conv2D(filters, (3, 3), padding='same',\n",
    "                 input_shape=ishape))\n",
    "  else:\n",
    "    model.add(Conv2D(filters, (3, 3), padding='same'))\n",
    "  \n",
    "  #model.add(BN())\n",
    "  #model.add(GN(0.3))\n",
    "  #model.add(Activation('relu'))\n",
    "  #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "  model.add(BN())\n",
    "  model.add(GN(0.1))\n",
    "  model.add(Activation('relu'))\n",
    "\n",
    "  model.add(Conv2D(filters, (3, 3), padding='same'))\n",
    "  model.add(BN())\n",
    "  model.add(GN(0.1))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHlVHAgKkmiy",
    "outputId": "375b8f1d-2cdb-4fbe-ab3b-ac0ae5046c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " gaussian_noise (GaussianNoi  (None, 32, 32, 32)       0         \n",
      " se)                                                             \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_1 (GaussianN  (None, 32, 32, 32)       0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_2 (GaussianN  (None, 16, 16, 64)       0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_3 (GaussianN  (None, 16, 16, 64)       0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_4 (GaussianN  (None, 8, 8, 128)        0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_5 (GaussianN  (None, 8, 8, 128)        0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_6 (GaussianN  (None, 4, 4, 256)        0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_7 (GaussianN  (None, 4, 4, 256)        0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 2, 2, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_8 (GaussianN  (None, 2, 2, 512)        0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 2, 2, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gaussian_noise_9 (GaussianN  (None, 2, 2, 512)        0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,987,946\n",
      "Trainable params: 4,983,978\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## DEF NN TOPOLOGY  \n",
    "model = Sequential()\n",
    "\n",
    "model=CBGN(model,32,x_train.shape[1:])\n",
    "model=CBGN(model,64)\n",
    "model=CBGN(model,128)\n",
    "model=CBGN(model,256)\n",
    "model=CBGN(model,512)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-S3kvjczpO8"
   },
   "outputs": [],
   "source": [
    "## OPTIM AND COMPILE\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "#opt = Adagrad\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, cooldown=1,\n",
    "                              patience=10, min_lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIeUuQHVktvJ",
    "outputId": "a319592f-9e71-4e0d-9d46-8191f1995c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "390/390 [==============================] - 52s 107ms/step - loss: 1.5774 - accuracy: 0.4214 - val_loss: 1.8186 - val_accuracy: 0.4363 - lr: 0.0010\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 1.1343 - accuracy: 0.5940 - val_loss: 1.1251 - val_accuracy: 0.6138 - lr: 0.0010\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.9459 - accuracy: 0.6669 - val_loss: 1.1457 - val_accuracy: 0.6049 - lr: 0.0010\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.8320 - accuracy: 0.7087 - val_loss: 0.7985 - val_accuracy: 0.7229 - lr: 0.0010\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.7536 - accuracy: 0.7391 - val_loss: 0.9503 - val_accuracy: 0.7004 - lr: 0.0010\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.6951 - accuracy: 0.7604 - val_loss: 0.8144 - val_accuracy: 0.7339 - lr: 0.0010\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.6542 - accuracy: 0.7741 - val_loss: 1.0918 - val_accuracy: 0.6760 - lr: 0.0010\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.6153 - accuracy: 0.7868 - val_loss: 0.6428 - val_accuracy: 0.7807 - lr: 0.0010\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.5837 - accuracy: 0.7974 - val_loss: 0.6591 - val_accuracy: 0.7755 - lr: 0.0010\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.5544 - accuracy: 0.8095 - val_loss: 0.6508 - val_accuracy: 0.7807 - lr: 0.0010\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.5357 - accuracy: 0.8165 - val_loss: 0.6430 - val_accuracy: 0.7905 - lr: 0.0010\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5110 - accuracy: 0.8237 - val_loss: 0.5964 - val_accuracy: 0.8038 - lr: 0.0010\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4916 - accuracy: 0.8307 - val_loss: 0.5857 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.4813 - accuracy: 0.8337 - val_loss: 0.7058 - val_accuracy: 0.7744 - lr: 0.0010\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.4616 - accuracy: 0.8390 - val_loss: 0.6313 - val_accuracy: 0.8034 - lr: 0.0010\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.4486 - accuracy: 0.8450 - val_loss: 0.6341 - val_accuracy: 0.7969 - lr: 0.0010\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.4338 - accuracy: 0.8518 - val_loss: 0.4974 - val_accuracy: 0.8310 - lr: 0.0010\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.4188 - accuracy: 0.8566 - val_loss: 0.5394 - val_accuracy: 0.8255 - lr: 0.0010\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.4094 - accuracy: 0.8589 - val_loss: 0.4561 - val_accuracy: 0.8507 - lr: 0.0010\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.3992 - accuracy: 0.8616 - val_loss: 0.5995 - val_accuracy: 0.8168 - lr: 0.0010\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.3923 - accuracy: 0.8653 - val_loss: 0.4438 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3792 - accuracy: 0.8694 - val_loss: 0.5389 - val_accuracy: 0.8329 - lr: 0.0010\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3707 - accuracy: 0.8723 - val_loss: 0.4617 - val_accuracy: 0.8469 - lr: 0.0010\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.3637 - accuracy: 0.8734 - val_loss: 0.5006 - val_accuracy: 0.8433 - lr: 0.0010\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 42s 106ms/step - loss: 0.3567 - accuracy: 0.8757 - val_loss: 0.4429 - val_accuracy: 0.8530 - lr: 0.0010\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3450 - accuracy: 0.8819 - val_loss: 0.4452 - val_accuracy: 0.8556 - lr: 0.0010\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.3382 - accuracy: 0.8819 - val_loss: 0.6717 - val_accuracy: 0.7942 - lr: 0.0010\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3263 - accuracy: 0.8864 - val_loss: 0.4358 - val_accuracy: 0.8557 - lr: 0.0010\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3242 - accuracy: 0.8865 - val_loss: 0.3998 - val_accuracy: 0.8684 - lr: 0.0010\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.3175 - accuracy: 0.8904 - val_loss: 0.4490 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.3130 - accuracy: 0.8925 - val_loss: 0.5556 - val_accuracy: 0.8327 - lr: 0.0010\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 40s 104ms/step - loss: 0.3091 - accuracy: 0.8931 - val_loss: 0.4211 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 33/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3047 - accuracy: 0.8946 - val_loss: 0.4827 - val_accuracy: 0.8465 - lr: 0.0010\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2966 - accuracy: 0.8967 - val_loss: 0.4468 - val_accuracy: 0.8618 - lr: 0.0010\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.2919 - accuracy: 0.8993 - val_loss: 0.4435 - val_accuracy: 0.8552 - lr: 0.0010\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 41s 106ms/step - loss: 0.2886 - accuracy: 0.8997 - val_loss: 0.5057 - val_accuracy: 0.8436 - lr: 0.0010\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.2812 - accuracy: 0.9020 - val_loss: 0.4743 - val_accuracy: 0.8531 - lr: 0.0010\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.2717 - accuracy: 0.9056 - val_loss: 0.4207 - val_accuracy: 0.8652 - lr: 0.0010\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.2757 - accuracy: 0.9026 - val_loss: 0.4214 - val_accuracy: 0.8686 - lr: 0.0010\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.2257 - accuracy: 0.9206 - val_loss: 0.3494 - val_accuracy: 0.8848 - lr: 5.0000e-04\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 41s 106ms/step - loss: 0.2173 - accuracy: 0.9246 - val_loss: 0.3363 - val_accuracy: 0.8954 - lr: 5.0000e-04\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.2080 - accuracy: 0.9268 - val_loss: 0.3749 - val_accuracy: 0.8823 - lr: 5.0000e-04\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.2097 - accuracy: 0.9262 - val_loss: 0.3499 - val_accuracy: 0.8927 - lr: 5.0000e-04\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.2051 - accuracy: 0.9277 - val_loss: 0.3514 - val_accuracy: 0.8899 - lr: 5.0000e-04\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2035 - accuracy: 0.9294 - val_loss: 0.3556 - val_accuracy: 0.8912 - lr: 5.0000e-04\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.1962 - accuracy: 0.9306 - val_loss: 0.3756 - val_accuracy: 0.8876 - lr: 5.0000e-04\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1946 - accuracy: 0.9310 - val_loss: 0.3842 - val_accuracy: 0.8840 - lr: 5.0000e-04\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.1943 - accuracy: 0.9316 - val_loss: 0.3395 - val_accuracy: 0.8939 - lr: 5.0000e-04\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1854 - accuracy: 0.9352 - val_loss: 0.3422 - val_accuracy: 0.8950 - lr: 5.0000e-04\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.1845 - accuracy: 0.9348 - val_loss: 0.3736 - val_accuracy: 0.8860 - lr: 5.0000e-04\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 41s 106ms/step - loss: 0.1826 - accuracy: 0.9353 - val_loss: 0.3845 - val_accuracy: 0.8829 - lr: 5.0000e-04\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1641 - accuracy: 0.9428 - val_loss: 0.3423 - val_accuracy: 0.8986 - lr: 2.5000e-04\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.1583 - accuracy: 0.9433 - val_loss: 0.3222 - val_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1532 - accuracy: 0.9450 - val_loss: 0.3149 - val_accuracy: 0.9061 - lr: 2.5000e-04\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.1523 - accuracy: 0.9466 - val_loss: 0.3432 - val_accuracy: 0.8980 - lr: 2.5000e-04\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1501 - accuracy: 0.9459 - val_loss: 0.3298 - val_accuracy: 0.9015 - lr: 2.5000e-04\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 42s 106ms/step - loss: 0.1466 - accuracy: 0.9476 - val_loss: 0.3462 - val_accuracy: 0.8978 - lr: 2.5000e-04\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.1441 - accuracy: 0.9484 - val_loss: 0.3368 - val_accuracy: 0.9032 - lr: 2.5000e-04\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.1434 - accuracy: 0.9485 - val_loss: 0.3442 - val_accuracy: 0.8988 - lr: 2.5000e-04\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.1423 - accuracy: 0.9499 - val_loss: 0.3480 - val_accuracy: 0.9006 - lr: 2.5000e-04\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.1455 - accuracy: 0.9482 - val_loss: 0.3452 - val_accuracy: 0.8986 - lr: 2.5000e-04\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1398 - accuracy: 0.9506 - val_loss: 0.3322 - val_accuracy: 0.9037 - lr: 2.5000e-04\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.1373 - accuracy: 0.9505 - val_loss: 0.3398 - val_accuracy: 0.9034 - lr: 2.5000e-04\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.1338 - accuracy: 0.9528 - val_loss: 0.3346 - val_accuracy: 0.9007 - lr: 2.5000e-04\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1306 - accuracy: 0.9537 - val_loss: 0.3352 - val_accuracy: 0.9052 - lr: 1.2500e-04\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.1238 - accuracy: 0.9562 - val_loss: 0.3422 - val_accuracy: 0.9033 - lr: 1.2500e-04\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1233 - accuracy: 0.9558 - val_loss: 0.3337 - val_accuracy: 0.9064 - lr: 1.2500e-04\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.1192 - accuracy: 0.9579 - val_loss: 0.3432 - val_accuracy: 0.9049 - lr: 1.2500e-04\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1185 - accuracy: 0.9589 - val_loss: 0.3585 - val_accuracy: 0.9021 - lr: 1.2500e-04\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.1203 - accuracy: 0.9573 - val_loss: 0.3471 - val_accuracy: 0.9045 - lr: 1.2500e-04\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.1207 - accuracy: 0.9572 - val_loss: 0.3460 - val_accuracy: 0.9051 - lr: 1.2500e-04\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.1191 - accuracy: 0.9575 - val_loss: 0.3464 - val_accuracy: 0.9037 - lr: 1.2500e-04\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 0.3432 - val_accuracy: 0.9056 - lr: 1.2500e-04\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.1138 - accuracy: 0.9593 - val_loss: 0.3581 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.1121 - accuracy: 0.9606 - val_loss: 0.3400 - val_accuracy: 0.9053 - lr: 6.2500e-05\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.1089 - accuracy: 0.9608 - val_loss: 0.3519 - val_accuracy: 0.9033 - lr: 6.2500e-05\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.1085 - accuracy: 0.9618 - val_loss: 0.3400 - val_accuracy: 0.9062 - lr: 6.2500e-05\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.1059 - accuracy: 0.9616 - val_loss: 0.3431 - val_accuracy: 0.9062 - lr: 6.2500e-05\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1073 - accuracy: 0.9622 - val_loss: 0.3475 - val_accuracy: 0.9047 - lr: 6.2500e-05\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 43s 111ms/step - loss: 0.1061 - accuracy: 0.9628 - val_loss: 0.3483 - val_accuracy: 0.9062 - lr: 6.2500e-05\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.1093 - accuracy: 0.9605 - val_loss: 0.3400 - val_accuracy: 0.9074 - lr: 6.2500e-05\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1037 - accuracy: 0.9624 - val_loss: 0.3535 - val_accuracy: 0.9059 - lr: 6.2500e-05\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.1077 - accuracy: 0.9612 - val_loss: 0.3408 - val_accuracy: 0.9085 - lr: 6.2500e-05\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1029 - accuracy: 0.9634 - val_loss: 0.3528 - val_accuracy: 0.9050 - lr: 6.2500e-05\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 40s 104ms/step - loss: 0.0999 - accuracy: 0.9650 - val_loss: 0.3525 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.1019 - accuracy: 0.9641 - val_loss: 0.3460 - val_accuracy: 0.9068 - lr: 5.0000e-05\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.1025 - accuracy: 0.9639 - val_loss: 0.3439 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.0983 - accuracy: 0.9647 - val_loss: 0.3414 - val_accuracy: 0.9084 - lr: 5.0000e-05\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.1012 - accuracy: 0.9631 - val_loss: 0.3422 - val_accuracy: 0.9082 - lr: 5.0000e-05\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.1005 - accuracy: 0.9636 - val_loss: 0.3459 - val_accuracy: 0.9078 - lr: 5.0000e-05\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.1000 - accuracy: 0.9634 - val_loss: 0.3504 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 92/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0991 - accuracy: 0.9645 - val_loss: 0.3429 - val_accuracy: 0.9092 - lr: 5.0000e-05\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.1000 - accuracy: 0.9641 - val_loss: 0.3541 - val_accuracy: 0.9060 - lr: 5.0000e-05\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.0986 - accuracy: 0.9654 - val_loss: 0.3488 - val_accuracy: 0.9076 - lr: 5.0000e-05\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.1000 - accuracy: 0.9647 - val_loss: 0.3508 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0982 - accuracy: 0.9651 - val_loss: 0.3569 - val_accuracy: 0.9057 - lr: 5.0000e-05\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.0990 - accuracy: 0.9647 - val_loss: 0.3594 - val_accuracy: 0.9073 - lr: 5.0000e-05\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0951 - accuracy: 0.9666 - val_loss: 0.3595 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0992 - accuracy: 0.9638 - val_loss: 0.3522 - val_accuracy: 0.9066 - lr: 5.0000e-05\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0968 - accuracy: 0.9653 - val_loss: 0.3584 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0963 - accuracy: 0.9658 - val_loss: 0.3550 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0951 - accuracy: 0.9666 - val_loss: 0.3516 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0942 - accuracy: 0.9671 - val_loss: 0.3554 - val_accuracy: 0.9087 - lr: 5.0000e-05\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0983 - accuracy: 0.9648 - val_loss: 0.3598 - val_accuracy: 0.9049 - lr: 5.0000e-05\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0981 - accuracy: 0.9648 - val_loss: 0.3557 - val_accuracy: 0.9077 - lr: 5.0000e-05\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0954 - accuracy: 0.9658 - val_loss: 0.3575 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0928 - accuracy: 0.9680 - val_loss: 0.3578 - val_accuracy: 0.9068 - lr: 5.0000e-05\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0971 - accuracy: 0.9657 - val_loss: 0.3659 - val_accuracy: 0.9054 - lr: 5.0000e-05\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0919 - accuracy: 0.9668 - val_loss: 0.3501 - val_accuracy: 0.9086 - lr: 5.0000e-05\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.0931 - accuracy: 0.9666 - val_loss: 0.3484 - val_accuracy: 0.9089 - lr: 5.0000e-05\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0932 - accuracy: 0.9668 - val_loss: 0.3613 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0917 - accuracy: 0.9670 - val_loss: 0.3566 - val_accuracy: 0.9066 - lr: 5.0000e-05\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0936 - accuracy: 0.9670 - val_loss: 0.3656 - val_accuracy: 0.9034 - lr: 5.0000e-05\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0935 - accuracy: 0.9666 - val_loss: 0.3575 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0937 - accuracy: 0.9666 - val_loss: 0.3589 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0955 - accuracy: 0.9662 - val_loss: 0.3537 - val_accuracy: 0.9077 - lr: 5.0000e-05\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0921 - accuracy: 0.9674 - val_loss: 0.3572 - val_accuracy: 0.9042 - lr: 5.0000e-05\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0910 - accuracy: 0.9669 - val_loss: 0.3607 - val_accuracy: 0.9051 - lr: 5.0000e-05\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0906 - accuracy: 0.9675 - val_loss: 0.3563 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 38s 99ms/step - loss: 0.0915 - accuracy: 0.9661 - val_loss: 0.3627 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0931 - accuracy: 0.9674 - val_loss: 0.3558 - val_accuracy: 0.9063 - lr: 5.0000e-05\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0887 - accuracy: 0.9688 - val_loss: 0.3617 - val_accuracy: 0.9062 - lr: 5.0000e-05\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0880 - accuracy: 0.9684 - val_loss: 0.3647 - val_accuracy: 0.9062 - lr: 5.0000e-05\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0928 - accuracy: 0.9672 - val_loss: 0.3600 - val_accuracy: 0.9077 - lr: 5.0000e-05\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0932 - accuracy: 0.9669 - val_loss: 0.3642 - val_accuracy: 0.9044 - lr: 5.0000e-05\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 38s 99ms/step - loss: 0.0890 - accuracy: 0.9682 - val_loss: 0.3682 - val_accuracy: 0.9043 - lr: 5.0000e-05\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0896 - accuracy: 0.9680 - val_loss: 0.3556 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0898 - accuracy: 0.9686 - val_loss: 0.3736 - val_accuracy: 0.9029 - lr: 5.0000e-05\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0876 - accuracy: 0.9687 - val_loss: 0.3640 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0899 - accuracy: 0.9678 - val_loss: 0.3657 - val_accuracy: 0.9073 - lr: 5.0000e-05\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0868 - accuracy: 0.9694 - val_loss: 0.3557 - val_accuracy: 0.9099 - lr: 5.0000e-05\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 39s 98ms/step - loss: 0.0910 - accuracy: 0.9675 - val_loss: 0.3582 - val_accuracy: 0.9082 - lr: 5.0000e-05\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0891 - accuracy: 0.9687 - val_loss: 0.3525 - val_accuracy: 0.9090 - lr: 5.0000e-05\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0855 - accuracy: 0.9692 - val_loss: 0.3588 - val_accuracy: 0.9086 - lr: 5.0000e-05\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 39s 98ms/step - loss: 0.0885 - accuracy: 0.9683 - val_loss: 0.3678 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0870 - accuracy: 0.9686 - val_loss: 0.3539 - val_accuracy: 0.9096 - lr: 5.0000e-05\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0881 - accuracy: 0.9692 - val_loss: 0.3687 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0865 - accuracy: 0.9695 - val_loss: 0.3596 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0872 - accuracy: 0.9692 - val_loss: 0.3658 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0876 - accuracy: 0.9683 - val_loss: 0.3653 - val_accuracy: 0.9064 - lr: 5.0000e-05\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0870 - accuracy: 0.9695 - val_loss: 0.3713 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0859 - accuracy: 0.9701 - val_loss: 0.3646 - val_accuracy: 0.9076 - lr: 5.0000e-05\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0860 - accuracy: 0.9699 - val_loss: 0.3594 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0839 - accuracy: 0.9705 - val_loss: 0.3672 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0857 - accuracy: 0.9704 - val_loss: 0.3740 - val_accuracy: 0.9056 - lr: 5.0000e-05\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0874 - accuracy: 0.9694 - val_loss: 0.3629 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0843 - accuracy: 0.9704 - val_loss: 0.3750 - val_accuracy: 0.9054 - lr: 5.0000e-05\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0825 - accuracy: 0.9711 - val_loss: 0.3661 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0857 - accuracy: 0.9696 - val_loss: 0.3662 - val_accuracy: 0.9073 - lr: 5.0000e-05\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0823 - accuracy: 0.9703 - val_loss: 0.3676 - val_accuracy: 0.9071 - lr: 5.0000e-05\n",
      "Epoch 151/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0802 - accuracy: 0.9713 - val_loss: 0.3718 - val_accuracy: 0.9056 - lr: 5.0000e-05\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0840 - accuracy: 0.9696 - val_loss: 0.3726 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0856 - accuracy: 0.9693 - val_loss: 0.3748 - val_accuracy: 0.9077 - lr: 5.0000e-05\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.0833 - accuracy: 0.9701 - val_loss: 0.3654 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0839 - accuracy: 0.9701 - val_loss: 0.3668 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0846 - accuracy: 0.9700 - val_loss: 0.3616 - val_accuracy: 0.9084 - lr: 5.0000e-05\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0806 - accuracy: 0.9710 - val_loss: 0.3688 - val_accuracy: 0.9081 - lr: 5.0000e-05\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0820 - accuracy: 0.9711 - val_loss: 0.3745 - val_accuracy: 0.9060 - lr: 5.0000e-05\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0847 - accuracy: 0.9696 - val_loss: 0.3704 - val_accuracy: 0.9058 - lr: 5.0000e-05\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0800 - accuracy: 0.9719 - val_loss: 0.3759 - val_accuracy: 0.9052 - lr: 5.0000e-05\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0816 - accuracy: 0.9711 - val_loss: 0.3706 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0820 - accuracy: 0.9717 - val_loss: 0.3716 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0805 - accuracy: 0.9715 - val_loss: 0.3712 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0838 - accuracy: 0.9699 - val_loss: 0.3708 - val_accuracy: 0.9090 - lr: 5.0000e-05\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0830 - accuracy: 0.9707 - val_loss: 0.3698 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0805 - accuracy: 0.9709 - val_loss: 0.3655 - val_accuracy: 0.9082 - lr: 5.0000e-05\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0800 - accuracy: 0.9716 - val_loss: 0.3688 - val_accuracy: 0.9082 - lr: 5.0000e-05\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0830 - accuracy: 0.9704 - val_loss: 0.3719 - val_accuracy: 0.9068 - lr: 5.0000e-05\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0785 - accuracy: 0.9718 - val_loss: 0.3844 - val_accuracy: 0.9049 - lr: 5.0000e-05\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0812 - accuracy: 0.9712 - val_loss: 0.3741 - val_accuracy: 0.9071 - lr: 5.0000e-05\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0794 - accuracy: 0.9719 - val_loss: 0.3718 - val_accuracy: 0.9076 - lr: 5.0000e-05\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0814 - accuracy: 0.9707 - val_loss: 0.3760 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0764 - accuracy: 0.9729 - val_loss: 0.3766 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0787 - accuracy: 0.9717 - val_loss: 0.3842 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0791 - accuracy: 0.9717 - val_loss: 0.3655 - val_accuracy: 0.9087 - lr: 5.0000e-05\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0798 - accuracy: 0.9709 - val_loss: 0.3743 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0801 - accuracy: 0.9711 - val_loss: 0.3738 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0776 - accuracy: 0.9726 - val_loss: 0.3755 - val_accuracy: 0.9079 - lr: 5.0000e-05\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0814 - accuracy: 0.9704 - val_loss: 0.3769 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0798 - accuracy: 0.9714 - val_loss: 0.3741 - val_accuracy: 0.9073 - lr: 5.0000e-05\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0771 - accuracy: 0.9728 - val_loss: 0.3714 - val_accuracy: 0.9076 - lr: 5.0000e-05\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0779 - accuracy: 0.9717 - val_loss: 0.3694 - val_accuracy: 0.9091 - lr: 5.0000e-05\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0786 - accuracy: 0.9718 - val_loss: 0.3831 - val_accuracy: 0.9080 - lr: 5.0000e-05\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0758 - accuracy: 0.9733 - val_loss: 0.3628 - val_accuracy: 0.9107 - lr: 5.0000e-05\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0769 - accuracy: 0.9731 - val_loss: 0.3789 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0768 - accuracy: 0.9736 - val_loss: 0.3773 - val_accuracy: 0.9079 - lr: 5.0000e-05\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0779 - accuracy: 0.9719 - val_loss: 0.3805 - val_accuracy: 0.9076 - lr: 5.0000e-05\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0751 - accuracy: 0.9730 - val_loss: 0.3834 - val_accuracy: 0.9080 - lr: 5.0000e-05\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0786 - accuracy: 0.9722 - val_loss: 0.3784 - val_accuracy: 0.9079 - lr: 5.0000e-05\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0770 - accuracy: 0.9720 - val_loss: 0.3775 - val_accuracy: 0.9094 - lr: 5.0000e-05\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0815 - accuracy: 0.9717 - val_loss: 0.3773 - val_accuracy: 0.9079 - lr: 5.0000e-05\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0785 - accuracy: 0.9727 - val_loss: 0.3793 - val_accuracy: 0.9056 - lr: 5.0000e-05\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0787 - accuracy: 0.9716 - val_loss: 0.3737 - val_accuracy: 0.9060 - lr: 5.0000e-05\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0753 - accuracy: 0.9733 - val_loss: 0.3688 - val_accuracy: 0.9088 - lr: 5.0000e-05\n",
      "Epoch 195/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0783 - accuracy: 0.9723 - val_loss: 0.3758 - val_accuracy: 0.9089 - lr: 5.0000e-05\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0749 - accuracy: 0.9737 - val_loss: 0.3781 - val_accuracy: 0.9090 - lr: 5.0000e-05\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0736 - accuracy: 0.9734 - val_loss: 0.3854 - val_accuracy: 0.9085 - lr: 5.0000e-05\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0777 - accuracy: 0.9733 - val_loss: 0.3734 - val_accuracy: 0.9088 - lr: 5.0000e-05\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0743 - accuracy: 0.9743 - val_loss: 0.3830 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0749 - accuracy: 0.9733 - val_loss: 0.3771 - val_accuracy: 0.9093 - lr: 5.0000e-05\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0750 - accuracy: 0.9732 - val_loss: 0.3869 - val_accuracy: 0.9081 - lr: 5.0000e-05\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0784 - accuracy: 0.9726 - val_loss: 0.3715 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0752 - accuracy: 0.9731 - val_loss: 0.3786 - val_accuracy: 0.9091 - lr: 5.0000e-05\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0722 - accuracy: 0.9749 - val_loss: 0.3869 - val_accuracy: 0.9061 - lr: 5.0000e-05\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0751 - accuracy: 0.9731 - val_loss: 0.3762 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0738 - accuracy: 0.9740 - val_loss: 0.3808 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0724 - accuracy: 0.9733 - val_loss: 0.3869 - val_accuracy: 0.9078 - lr: 5.0000e-05\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0764 - accuracy: 0.9728 - val_loss: 0.3810 - val_accuracy: 0.9082 - lr: 5.0000e-05\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0743 - accuracy: 0.9728 - val_loss: 0.3820 - val_accuracy: 0.9054 - lr: 5.0000e-05\n",
      "Epoch 210/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0733 - accuracy: 0.9744 - val_loss: 0.3813 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0735 - accuracy: 0.9738 - val_loss: 0.3777 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0751 - accuracy: 0.9732 - val_loss: 0.3786 - val_accuracy: 0.9078 - lr: 5.0000e-05\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.0726 - accuracy: 0.9740 - val_loss: 0.3767 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.0716 - accuracy: 0.9747 - val_loss: 0.3822 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0727 - accuracy: 0.9741 - val_loss: 0.3873 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0728 - accuracy: 0.9744 - val_loss: 0.3786 - val_accuracy: 0.9079 - lr: 5.0000e-05\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0708 - accuracy: 0.9746 - val_loss: 0.3829 - val_accuracy: 0.9084 - lr: 5.0000e-05\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0726 - accuracy: 0.9746 - val_loss: 0.3873 - val_accuracy: 0.9071 - lr: 5.0000e-05\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0731 - accuracy: 0.9734 - val_loss: 0.3828 - val_accuracy: 0.9055 - lr: 5.0000e-05\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0744 - accuracy: 0.9742 - val_loss: 0.3794 - val_accuracy: 0.9061 - lr: 5.0000e-05\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0743 - accuracy: 0.9733 - val_loss: 0.3864 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0724 - accuracy: 0.9747 - val_loss: 0.3851 - val_accuracy: 0.9076 - lr: 5.0000e-05\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0708 - accuracy: 0.9743 - val_loss: 0.3851 - val_accuracy: 0.9071 - lr: 5.0000e-05\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.0740 - accuracy: 0.9731 - val_loss: 0.3884 - val_accuracy: 0.9077 - lr: 5.0000e-05\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0718 - accuracy: 0.9750 - val_loss: 0.3814 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0746 - accuracy: 0.9737 - val_loss: 0.3789 - val_accuracy: 0.9070 - lr: 5.0000e-05\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0711 - accuracy: 0.9744 - val_loss: 0.3879 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0705 - accuracy: 0.9759 - val_loss: 0.3863 - val_accuracy: 0.9066 - lr: 5.0000e-05\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0699 - accuracy: 0.9753 - val_loss: 0.3877 - val_accuracy: 0.9065 - lr: 5.0000e-05\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0679 - accuracy: 0.9760 - val_loss: 0.3878 - val_accuracy: 0.9073 - lr: 5.0000e-05\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0708 - accuracy: 0.9747 - val_loss: 0.3824 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0713 - accuracy: 0.9748 - val_loss: 0.3914 - val_accuracy: 0.9074 - lr: 5.0000e-05\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.0696 - accuracy: 0.9750 - val_loss: 0.3835 - val_accuracy: 0.9089 - lr: 5.0000e-05\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0675 - accuracy: 0.9758 - val_loss: 0.3917 - val_accuracy: 0.9069 - lr: 5.0000e-05\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0723 - accuracy: 0.9743 - val_loss: 0.3907 - val_accuracy: 0.9061 - lr: 5.0000e-05\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0679 - accuracy: 0.9757 - val_loss: 0.3864 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0700 - accuracy: 0.9759 - val_loss: 0.3821 - val_accuracy: 0.9095 - lr: 5.0000e-05\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 38s 97ms/step - loss: 0.0704 - accuracy: 0.9745 - val_loss: 0.3935 - val_accuracy: 0.9067 - lr: 5.0000e-05\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0695 - accuracy: 0.9756 - val_loss: 0.3868 - val_accuracy: 0.9075 - lr: 5.0000e-05\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0686 - accuracy: 0.9756 - val_loss: 0.3808 - val_accuracy: 0.9092 - lr: 5.0000e-05\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0692 - accuracy: 0.9756 - val_loss: 0.3908 - val_accuracy: 0.9066 - lr: 5.0000e-05\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.0678 - accuracy: 0.9762 - val_loss: 0.3873 - val_accuracy: 0.9086 - lr: 5.0000e-05\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0670 - accuracy: 0.9761 - val_loss: 0.3866 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0686 - accuracy: 0.9757 - val_loss: 0.3781 - val_accuracy: 0.9097 - lr: 5.0000e-05\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0658 - accuracy: 0.9762 - val_loss: 0.3852 - val_accuracy: 0.9083 - lr: 5.0000e-05\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 39s 99ms/step - loss: 0.0661 - accuracy: 0.9761 - val_loss: 0.3859 - val_accuracy: 0.9105 - lr: 5.0000e-05\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0702 - accuracy: 0.9759 - val_loss: 0.3951 - val_accuracy: 0.9081 - lr: 5.0000e-05\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.0687 - accuracy: 0.9753 - val_loss: 0.3906 - val_accuracy: 0.9078 - lr: 5.0000e-05\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.0668 - accuracy: 0.9765 - val_loss: 0.3831 - val_accuracy: 0.9086 - lr: 5.0000e-05\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 38s 98ms/step - loss: 0.0672 - accuracy: 0.9766 - val_loss: 0.3888 - val_accuracy: 0.9074 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "## TRAINING with DA and LRA\n",
    "history=model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                            steps_per_epoch=len(x_train) / batch_size, \n",
    "                            epochs=epochs,\n",
    "                            #validation_data=(x_test, y_test),\n",
    "                            validation_data=testdatagen.flow(x_test, y_test),\n",
    "                            callbacks=[reduce_lr],\n",
    "                            verbose=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
