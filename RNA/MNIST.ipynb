{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cul-LMkIN6y4"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAO7azh1PBMh"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 #coge 128 muestas\n",
        "num_classes = 10\n",
        "epochs = 200 #numero de iteraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMoBlCxcPIIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ce09a5-ea66-413c-d9d6-875a44c759b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test,y_test)=mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLzvWLENQZ-R"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "x_train= x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyqHtSuIRKsa"
      },
      "outputs": [],
      "source": [
        "#Normalizar\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv5CSHyQRTGh",
        "outputId": "67588884-27f9-4478-c7e8-b1f7b7e8d85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2nnuRavRkYp"
      },
      "outputs": [],
      "source": [
        "#convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-rGPKfZ2Bxq"
      },
      "outputs": [],
      "source": [
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivh_YFDJTVMp"
      },
      "outputs": [],
      "source": [
        "#model = Sequential()\n",
        "#model.add(Dense(512,activation = 'relu',input_shape =(784,)))\n",
        "#model.add(Dense(512,activation = 'relu'))\n",
        "#model.add(Dense(512,activation = 'relu'))\n",
        "#model.add(Dense(num_classes,activation = 'softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibRA00GRgFy5"
      },
      "outputs": [],
      "source": [
        "#model = Sequential()\n",
        "#model.add(Dense(1024,activation = 'relu',input_shape =(784,)))\n",
        "#model.add(Dense(512,activation = 'relu'))\n",
        "#model.add(Dense(512,activation = 'relu'))\n",
        "#model.add(Dense(128,activation = 'relu'))\n",
        "#model.add(Dense(num_classes,activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PC27FU1RfLe"
      },
      "outputs": [],
      "source": [
        "## NN with BN\n",
        "model = Sequential()\n",
        "model.add(Reshape(target_shape=(784,), input_shape=(28,28,1)))\n",
        "model.add(GN(0.1))\n",
        "\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(BN())\n",
        "model.add(GN(0.1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt9u86ZqANMm"
      },
      "outputs": [],
      "source": [
        "#model = Sequential()\n",
        "#model.add(Reshape(target_shape=(784,), input_shape=(28,28,1)))\n",
        "#model.add(GN(0.3))\n",
        "\n",
        "#model.add(Dense(512))\n",
        "#model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "#model.add(Activation('relu'))\n",
        "\n",
        "#model.add(Dense(512))\n",
        "#model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "#model.add(Activation('relu'))\n",
        "\n",
        "#model.add(Dense(512))\n",
        "#model.add(BN())\n",
        "#model.add(GN(0.3))\n",
        "#model.add(Activation('relu'))\n",
        "\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j3zfuvkV5rx",
        "outputId": "6bcb64d7-bd45-4067-c7b5-4f0e4fb9926d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 784)               0         \n",
            "                                                                 \n",
            " gaussian_noise (GaussianNoi  (None, 784)              0         \n",
            " se)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1024)             4096      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " gaussian_noise_1 (GaussianN  (None, 1024)             0         \n",
            " oise)                                                           \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " gaussian_noise_2 (GaussianN  (None, 1024)             0         \n",
            " oise)                                                           \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " gaussian_noise_3 (GaussianN  (None, 1024)             0         \n",
            " oise)                                                           \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-0wSygSV82F"
      },
      "outputs": [],
      "source": [
        "#sgd = SGD(lr=0.01,decay=1e-6,momentum=0.9)\n",
        "\n",
        "sgd = SGD(learning_rate=0.1, decay=0.0, momentum=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg4_eXFHmf6N"
      },
      "outputs": [],
      "source": [
        "## define a learning rate scheduler\n",
        "def scheduler(epoch):\n",
        "    if epoch < 25:\n",
        "        return .1\n",
        "    elif epoch < 50:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRpLfmk7WIbZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer = sgd,metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yelohHs2QUc",
        "outputId": "cb13cfaa-8ea6-4cf2-f070-d822eda7028d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "937/937 [==============================] - 81s 83ms/step - loss: 0.3494 - accuracy: 0.8937 - val_loss: 0.1576 - val_accuracy: 0.9520 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.1869 - accuracy: 0.9433 - val_loss: 0.0935 - val_accuracy: 0.9698 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "937/937 [==============================] - 72s 76ms/step - loss: 0.1483 - accuracy: 0.9541 - val_loss: 0.0850 - val_accuracy: 0.9733 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.1256 - accuracy: 0.9613 - val_loss: 0.0651 - val_accuracy: 0.9794 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "937/937 [==============================] - 71s 76ms/step - loss: 0.1145 - accuracy: 0.9640 - val_loss: 0.0654 - val_accuracy: 0.9800 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "937/937 [==============================] - 71s 76ms/step - loss: 0.1032 - accuracy: 0.9669 - val_loss: 0.0729 - val_accuracy: 0.9786 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0974 - accuracy: 0.9700 - val_loss: 0.0528 - val_accuracy: 0.9843 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0916 - accuracy: 0.9722 - val_loss: 0.0490 - val_accuracy: 0.9837 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0858 - accuracy: 0.9726 - val_loss: 0.0505 - val_accuracy: 0.9838 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0815 - accuracy: 0.9747 - val_loss: 0.0401 - val_accuracy: 0.9865 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "937/937 [==============================] - 70s 75ms/step - loss: 0.0753 - accuracy: 0.9752 - val_loss: 0.0416 - val_accuracy: 0.9873 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0695 - accuracy: 0.9779 - val_loss: 0.0427 - val_accuracy: 0.9856 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0695 - accuracy: 0.9782 - val_loss: 0.0377 - val_accuracy: 0.9877 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "937/937 [==============================] - 72s 76ms/step - loss: 0.0666 - accuracy: 0.9788 - val_loss: 0.0325 - val_accuracy: 0.9890 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "937/937 [==============================] - 72s 76ms/step - loss: 0.0664 - accuracy: 0.9790 - val_loss: 0.0436 - val_accuracy: 0.9869 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0614 - accuracy: 0.9811 - val_loss: 0.0357 - val_accuracy: 0.9878 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0609 - accuracy: 0.9803 - val_loss: 0.0393 - val_accuracy: 0.9877 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "937/937 [==============================] - 71s 76ms/step - loss: 0.0611 - accuracy: 0.9813 - val_loss: 0.0415 - val_accuracy: 0.9879 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0563 - accuracy: 0.9820 - val_loss: 0.0435 - val_accuracy: 0.9850 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0548 - accuracy: 0.9825 - val_loss: 0.0340 - val_accuracy: 0.9879 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "937/937 [==============================] - 74s 78ms/step - loss: 0.0535 - accuracy: 0.9834 - val_loss: 0.0335 - val_accuracy: 0.9882 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0534 - accuracy: 0.9830 - val_loss: 0.0378 - val_accuracy: 0.9874 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "937/937 [==============================] - 74s 78ms/step - loss: 0.0472 - accuracy: 0.9847 - val_loss: 0.0335 - val_accuracy: 0.9884 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0490 - accuracy: 0.9841 - val_loss: 0.0417 - val_accuracy: 0.9873 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0485 - accuracy: 0.9844 - val_loss: 0.0325 - val_accuracy: 0.9891 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0408 - accuracy: 0.9868 - val_loss: 0.0273 - val_accuracy: 0.9908 - lr: 0.0100\n",
            "Epoch 27/200\n",
            "937/937 [==============================] - 73s 77ms/step - loss: 0.0364 - accuracy: 0.9884 - val_loss: 0.0260 - val_accuracy: 0.9918 - lr: 0.0100\n",
            "Epoch 28/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.0255 - val_accuracy: 0.9912 - lr: 0.0100\n",
            "Epoch 29/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0320 - accuracy: 0.9896 - val_loss: 0.0243 - val_accuracy: 0.9919 - lr: 0.0100\n",
            "Epoch 30/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 0.0245 - val_accuracy: 0.9925 - lr: 0.0100\n",
            "Epoch 31/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0294 - accuracy: 0.9905 - val_loss: 0.0248 - val_accuracy: 0.9921 - lr: 0.0100\n",
            "Epoch 32/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.0246 - val_accuracy: 0.9924 - lr: 0.0100\n",
            "Epoch 33/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0239 - val_accuracy: 0.9926 - lr: 0.0100\n",
            "Epoch 34/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0298 - accuracy: 0.9907 - val_loss: 0.0241 - val_accuracy: 0.9919 - lr: 0.0100\n",
            "Epoch 35/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.0240 - val_accuracy: 0.9922 - lr: 0.0100\n",
            "Epoch 36/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.0233 - val_accuracy: 0.9925 - lr: 0.0100\n",
            "Epoch 37/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.0230 - val_accuracy: 0.9924 - lr: 0.0100\n",
            "Epoch 38/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0279 - accuracy: 0.9912 - val_loss: 0.0242 - val_accuracy: 0.9920 - lr: 0.0100\n",
            "Epoch 39/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0279 - accuracy: 0.9908 - val_loss: 0.0239 - val_accuracy: 0.9922 - lr: 0.0100\n",
            "Epoch 40/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 0.0235 - val_accuracy: 0.9926 - lr: 0.0100\n",
            "Epoch 41/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0259 - accuracy: 0.9919 - val_loss: 0.0236 - val_accuracy: 0.9921 - lr: 0.0100\n",
            "Epoch 42/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.0239 - val_accuracy: 0.9922 - lr: 0.0100\n",
            "Epoch 43/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.0226 - val_accuracy: 0.9923 - lr: 0.0100\n",
            "Epoch 44/200\n",
            "937/937 [==============================] - 73s 77ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0231 - val_accuracy: 0.9920 - lr: 0.0100\n",
            "Epoch 45/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0255 - accuracy: 0.9924 - val_loss: 0.0228 - val_accuracy: 0.9928 - lr: 0.0100\n",
            "Epoch 46/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0259 - accuracy: 0.9915 - val_loss: 0.0227 - val_accuracy: 0.9929 - lr: 0.0100\n",
            "Epoch 47/200\n",
            "937/937 [==============================] - 75s 79ms/step - loss: 0.0264 - accuracy: 0.9919 - val_loss: 0.0230 - val_accuracy: 0.9926 - lr: 0.0100\n",
            "Epoch 48/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 0.0227 - val_accuracy: 0.9925 - lr: 0.0100\n",
            "Epoch 49/200\n",
            "937/937 [==============================] - 73s 77ms/step - loss: 0.0257 - accuracy: 0.9917 - val_loss: 0.0235 - val_accuracy: 0.9924 - lr: 0.0100\n",
            "Epoch 50/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.0232 - val_accuracy: 0.9924 - lr: 0.0100\n",
            "Epoch 51/200\n",
            "937/937 [==============================] - 71s 76ms/step - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.0228 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.0228 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.0227 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.0228 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.0225 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0229 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "937/937 [==============================] - 75s 81ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0223 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.0224 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.0227 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0250 - accuracy: 0.9922 - val_loss: 0.0225 - val_accuracy: 0.9925 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.0226 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0239 - accuracy: 0.9925 - val_loss: 0.0224 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.0227 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0242 - accuracy: 0.9921 - val_loss: 0.0222 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.0226 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0240 - accuracy: 0.9928 - val_loss: 0.0224 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0237 - accuracy: 0.9924 - val_loss: 0.0223 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 68/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0234 - accuracy: 0.9925 - val_loss: 0.0223 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 69/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0223 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 70/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.0222 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 71/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0234 - accuracy: 0.9929 - val_loss: 0.0223 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 72/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0230 - accuracy: 0.9931 - val_loss: 0.0222 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 73/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0237 - accuracy: 0.9927 - val_loss: 0.0221 - val_accuracy: 0.9925 - lr: 0.0010\n",
            "Epoch 74/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0242 - accuracy: 0.9924 - val_loss: 0.0224 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 75/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.0220 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 76/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.0224 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 77/200\n",
            "937/937 [==============================] - 73s 77ms/step - loss: 0.0237 - accuracy: 0.9921 - val_loss: 0.0221 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 78/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.0222 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 79/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0247 - accuracy: 0.9922 - val_loss: 0.0222 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 80/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0223 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 81/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0225 - accuracy: 0.9932 - val_loss: 0.0223 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 82/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0237 - accuracy: 0.9926 - val_loss: 0.0222 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 83/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0224 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 84/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 85/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 86/200\n",
            "937/937 [==============================] - 78s 83ms/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 87/200\n",
            "937/937 [==============================] - 74s 78ms/step - loss: 0.0241 - accuracy: 0.9925 - val_loss: 0.0220 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 88/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0221 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 89/200\n",
            "937/937 [==============================] - 77s 83ms/step - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.0221 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 90/200\n",
            "937/937 [==============================] - 73s 77ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0219 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 91/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0223 - accuracy: 0.9935 - val_loss: 0.0220 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 92/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.0218 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0225 - accuracy: 0.9929 - val_loss: 0.0219 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.0220 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0219 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0229 - accuracy: 0.9921 - val_loss: 0.0220 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "937/937 [==============================] - 76s 82ms/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "937/937 [==============================] - 72s 76ms/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0222 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0219 - accuracy: 0.9932 - val_loss: 0.0220 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0228 - accuracy: 0.9928 - val_loss: 0.0220 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 101/200\n",
            "937/937 [==============================] - 74s 78ms/step - loss: 0.0231 - accuracy: 0.9926 - val_loss: 0.0221 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 102/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 103/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.0219 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 104/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 105/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0218 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 106/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0232 - accuracy: 0.9925 - val_loss: 0.0220 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 107/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0219 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 108/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0217 - accuracy: 0.9933 - val_loss: 0.0217 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 109/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0225 - accuracy: 0.9928 - val_loss: 0.0219 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 110/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0218 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 111/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0233 - accuracy: 0.9930 - val_loss: 0.0220 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 112/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.0219 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 113/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.0217 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 114/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 115/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.0218 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 116/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 117/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0220 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 118/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0218 - accuracy: 0.9929 - val_loss: 0.0219 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 119/200\n",
            "937/937 [==============================] - 75s 79ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0220 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 120/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0232 - accuracy: 0.9931 - val_loss: 0.0219 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 121/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.0220 - val_accuracy: 0.9927 - lr: 0.0010\n",
            "Epoch 122/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.0221 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 123/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0218 - accuracy: 0.9929 - val_loss: 0.0220 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 124/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.0219 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 125/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0216 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 126/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 127/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.0220 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 128/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.0215 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 129/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0218 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 130/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0223 - accuracy: 0.9928 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 131/200\n",
            "937/937 [==============================] - 79s 84ms/step - loss: 0.0226 - accuracy: 0.9925 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 132/200\n",
            "937/937 [==============================] - 74s 78ms/step - loss: 0.0225 - accuracy: 0.9929 - val_loss: 0.0216 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 133/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.0216 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 134/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0236 - accuracy: 0.9928 - val_loss: 0.0219 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 135/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0218 - accuracy: 0.9927 - val_loss: 0.0214 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 136/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 137/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0219 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 138/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0227 - accuracy: 0.9927 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 139/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0234 - accuracy: 0.9926 - val_loss: 0.0216 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 140/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 0.0217 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 141/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.0215 - val_accuracy: 0.9926 - lr: 0.0010\n",
            "Epoch 142/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.0215 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 143/200\n",
            "937/937 [==============================] - 78s 84ms/step - loss: 0.0228 - accuracy: 0.9932 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 144/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0218 - accuracy: 0.9930 - val_loss: 0.0216 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 145/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0234 - accuracy: 0.9928 - val_loss: 0.0217 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 146/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.0216 - val_accuracy: 0.9928 - lr: 0.0010\n",
            "Epoch 147/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.0214 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 148/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0216 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 149/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.0212 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 150/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 151/200\n",
            "937/937 [==============================] - 76s 82ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0215 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 152/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0215 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 153/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0216 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 154/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.0215 - val_accuracy: 0.9932 - lr: 0.0010\n",
            "Epoch 155/200\n",
            "937/937 [==============================] - 77s 83ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0215 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 156/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.0213 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 157/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 0.0214 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 158/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 159/200\n",
            "937/937 [==============================] - 75s 81ms/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 160/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.0220 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 161/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0223 - accuracy: 0.9930 - val_loss: 0.0215 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 162/200\n",
            "937/937 [==============================] - 74s 78ms/step - loss: 0.0210 - accuracy: 0.9931 - val_loss: 0.0218 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 163/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0209 - accuracy: 0.9933 - val_loss: 0.0217 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 164/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0216 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 165/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0213 - accuracy: 0.9933 - val_loss: 0.0216 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 166/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0215 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 167/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0214 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 168/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0214 - accuracy: 0.9932 - val_loss: 0.0216 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 169/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.0216 - val_accuracy: 0.9932 - lr: 0.0010\n",
            "Epoch 170/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.0215 - val_accuracy: 0.9933 - lr: 0.0010\n",
            "Epoch 171/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0227 - accuracy: 0.9930 - val_loss: 0.0215 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 172/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.0215 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 173/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 0.0216 - val_accuracy: 0.9932 - lr: 0.0010\n",
            "Epoch 174/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0220 - accuracy: 0.9930 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 175/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.0211 - val_accuracy: 0.9932 - lr: 0.0010\n",
            "Epoch 176/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0212 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 177/200\n",
            "937/937 [==============================] - 79s 84ms/step - loss: 0.0226 - accuracy: 0.9928 - val_loss: 0.0212 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 178/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.0213 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 179/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0213 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 180/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.0214 - val_accuracy: 0.9932 - lr: 0.0010\n",
            "Epoch 181/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.0212 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 182/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0211 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 183/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.0214 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 184/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 185/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0214 - accuracy: 0.9936 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 186/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.0214 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 187/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.0213 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 188/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0213 - val_accuracy: 0.9933 - lr: 0.0010\n",
            "Epoch 189/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0211 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 190/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0215 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 191/200\n",
            "937/937 [==============================] - 72s 77ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0213 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 192/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0217 - accuracy: 0.9933 - val_loss: 0.0214 - val_accuracy: 0.9931 - lr: 0.0010\n",
            "Epoch 193/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.0211 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 194/200\n",
            "937/937 [==============================] - 75s 79ms/step - loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.0214 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 195/200\n",
            "937/937 [==============================] - 73s 78ms/step - loss: 0.0212 - accuracy: 0.9932 - val_loss: 0.0214 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 196/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.0214 - val_accuracy: 0.9930 - lr: 0.0010\n",
            "Epoch 197/200\n",
            "937/937 [==============================] - 76s 81ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0210 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 198/200\n",
            "937/937 [==============================] - 77s 82ms/step - loss: 0.0213 - accuracy: 0.9934 - val_loss: 0.0213 - val_accuracy: 0.9932 - lr: 0.0010\n",
            "Epoch 199/200\n",
            "937/937 [==============================] - 75s 80ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.0212 - val_accuracy: 0.9929 - lr: 0.0010\n",
            "Epoch 200/200\n",
            "937/937 [==============================] - 74s 79ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0212 - val_accuracy: 0.9931 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(datagen.flow\n",
        "                  (x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr],\n",
        "                            verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHpEMzcAYbn_"
      },
      "outputs": [],
      "source": [
        "#history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs,verbose = 1,validation_data=(x_test,y_test),callbacks=[set_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxvaJS5NYwR_",
        "outputId": "eca547f8-b0ba-452a-ecf5-8073fa29f3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
          ]
        }
      ],
      "source": [
        "print(history.history.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXVKPyvbA24q"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zKStPBxA9Kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a76c925-d46a-4478-fdac-d69a8927c923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.021211015060544014\n",
            "Test accuracy: 0.9930999875068665\n"
          ]
        }
      ],
      "source": [
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVcyvguOfoQI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnZRlO3efqyF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "edcf061c-917a-4ec6-cb25-b2c77481f5a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyzRjBXyfslC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}